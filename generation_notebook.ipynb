{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba75f6a9bc804f5287943db428de4bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "from logger import setup_logger\n",
    "\n",
    "# Initialize logger for notebook\n",
    "logger = setup_logger(name=\"NotebookLogger\", notebook=True)\n",
    "logger.info(\"Notebook execution started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Setting up LLM and JobApplicationBuilder.\n",
      "INFO - Extracting job content from the provided file.\n",
      "INFO - Job Details JSON generated at: output_files/Amazon/JrAppliedScient_20250526141836/JD.json\n",
      "INFO - Job content extracted successfully.\n",
      "INFO - Extracting user data from the provided resume file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'personal': {'name': 'Aaron Dankert', 'phone': '(505)507-8207', 'email': 'aarondankert@gmail.com', 'linkedin': 'https://www.linkedin.com/in/aadank/', 'github': 'https://github.com/aa-dank'}, 'summary': None, 'experiences': {'work_experience': [{'role': 'Systems Analyst', 'company': 'University of California, Santa Cruz', 'location': 'Santa Cruz, CA, USA', 'from_date': '2023', 'to_date': 'Present', 'description': ['Enhanced the department’s online presence by managing multiple websites and developing new web assets, resulting in increased user engagement.', 'Led a successful migration from CMiC Project Management Software to FileMaker, authoring scripts for data extraction and integration, which improved data workflows and accuracy.', 'Authored over 10,000 words of technical documentation and implemented advanced data cleaning techniques, facilitating improved process understanding, compliance, and data integrity across teams.']}, {'role': 'Archives Manager', 'company': 'University of California, Santa Cruz', 'location': 'Santa Cruz, CA, USA', 'from_date': '2020', 'to_date': '2023', 'description': ['Streamlined file management and access by developing a comprehensive Flask-based archives application, reducing retrieval overhead by 40%.', 'Enhanced database accuracy and security by implementing data scraping and Google authentication, ensuring compliance and robust data protection.', 'Led and trained a team of six employees on data management best practices and software tools, increasing team efficiency through targeted support and fostering a collaborative environment.']}, {'role': 'Data Coordinator', 'company': 'University of California, Santa Cruz', 'location': 'Santa Cruz, CA, USA', 'from_date': '2017', 'to_date': '2020', 'description': ['Supported over $900 million in capital projects by providing project engineering support, ensuring regulatory compliance, and enhancing system performance through effective data management.', 'Improved data quality and integrity by leveraging and optimizing existing tools, ensuring 100% compliance with regulatory standards and managing sensitive data securely.', 'Generated actionable insights through advanced statistical data processing and optimized financial tracking and reporting workflows, improving departmental efficiency.']}]}, 'projects': {'projects': [{'name': 'LLM Attribution: Challenges and Insights Across Model Stochasticity', 'type': None, 'link': '', 'resources': None, 'from_date': '2023', 'to_date': '2025', 'description': ['Engineered a text classification system by implementing Random Forest and Logistic Regression models with scikit-learn and PyTorch, improving LLM-generated text attribution accuracy.', 'Developed scalable data processing pipelines integrating BERT embeddings with custom stylometric features, enabling robust multi-class attribution and efficient large-scale data handling.', 'Conducted advanced data analysis and visualization using t-SNE, PCA, and statistical methods, generating actionable insights and comprehensive model performance reports.']}, {'name': 'Archives App', 'type': None, 'link': '', 'resources': None, 'from_date': '', 'to_date': '', 'description': ['Developed a data-driven archives application utilizing Python, Flask, and PostgreSQL, streamlining archival file management and enhancing data organization.', 'Integrated Google authentication, role-based access control, and Redis for background tasks, improving scalability, responsiveness, and organizational compliance.', 'Designed RESTful API endpoints and intuitive web interfaces, automating data interactions and reducing manual handling by 60%.']}, {'name': 'Archiving Desktop Application', 'type': None, 'link': '', 'resources': None, 'from_date': 'Sep 2023', 'to_date': 'Dec 2023', 'description': ['Developed a standalone desktop application with an intuitive PySimpleGUI interface and SQLite integration, improving document archiving workflow efficiency by over 50%.', 'Implemented robust backup solutions and automated file tracking, reducing data loss risk, enhancing data security, and decreasing manual archiving time by approximately 35%.', 'Enhanced tracking and reporting accuracy by 25% through comprehensive process automation and optimized data management.']}, {'name': 'LLM Research Benchmarking Platform', 'type': None, 'link': '', 'resources': None, 'from_date': '2024', 'to_date': '2025', 'description': ['Architected a scalable data pipeline extracting research papers from five academic databases, engineered a PostgreSQL database with vector embedding storage for 10,000+ papers, and enabled semantic search with pgvector.', 'Implemented advanced RAG (Retrieval-Augmented Generation) architecture and evaluation frameworks, reducing processing time by 40% and enhancing LLM scientific evaluation capabilities.', 'Developed interactive dashboards and comprehensive error analysis protocols, enabling reproducible benchmarking and actionable insights across scientific domains.']}, {'name': 'Job Hunter Toolbox', 'type': None, 'link': '', 'resources': None, 'from_date': '2024', 'to_date': '', 'description': ['Architected an automated resume generation system using Python, LaTeX, and LLM services, reducing application preparation time by nearly 60%.', 'Engineered a modular resume management system leveraging Jinja2 templates and LaTeX for pixel-perfect PDF generation and consistent formatting.', 'Implemented intelligent content tailoring and automated keyword optimization using LLM APIs, enhancing resume ATS compatibility and application relevance.']}]}, 'educations': {'education': [{'degree': 'Masters in Data Science', 'university': 'University of Michigan, Ann Arbor, USA', 'from_date': '2023', 'to_date': '2025', 'courses': []}, {'degree': 'Bachelors in Psychology and Economics', 'university': 'University of New Mexico, Albuquerque, USA', 'from_date': '2006', 'to_date': '2010', 'courses': []}]}, 'skills': {'skill_section': [{'name': 'Programming Languages', 'skills': ['Python', 'R', 'SQL', 'Typescript', 'Javascript', 'C++']}, {'name': 'Data Science & Machine Learning', 'skills': ['Pandas', 'Numpy', 'Scikit-Learn', 't-SNE', 'Principal Component Analysis', 'Pyspark', 'Statistical Analysis', 'Forecasting']}, {'name': 'Visualization', 'skills': ['Seaborn', 'Matplotlib', 'Altair', 'Jupyter Notebooks']}, {'name': 'Database Technologies', 'skills': ['Postgresql', 'SQLAlchemy', 'Redis', 'Sqlite']}, {'name': 'Operating Systems', 'skills': ['CentOS', 'Ubuntu', 'Windows', 'MacOS']}, {'name': 'Language Models & NLP', 'skills': ['OpenAI ChatGPT', 'NLTK', 'BERT']}, {'name': 'Tools & Technologies', 'skills': ['Flask', 'Jupyter Notebooks', 'Pyspark']}, {'name': 'Cloud & DevOps', 'skills': ['AWS', 'Azure Cognitive Services', 'Azure Services', 'Azure Cognitive and Generative AI', 'GCP', 'Cloud', 'Security Engineering', 'Security Architecture', 'DevOps', 'CI/CD']}, {'name': 'Software Development & Full Stack', 'skills': ['Java', 'React', 'JavaScript', 'Frontend', 'Backend', 'HTML', 'CSS', 'API', 'Angular', 'Full Stack Software Development']}, {'name': 'AI/ML Engineering', 'skills': ['Artificial Intelligence', 'Machine Learning', 'Deep Learning', 'Pretrained Models', 'AutoML', 'Data Extraction', 'Data Transformation', 'Data Loading', 'ETL', 'Data Modeling', 'Data Warehouse']}, {'name': 'Data Governance & Security', 'skills': ['Data Quality', 'Data Governance', 'Data Privacy', 'Compliance', 'SLA', 'Security', 'Privacy']}, {'name': 'Soft Skills', 'skills': ['Problem-solving', 'Communication skills', 'Cross-functional Collaboration', 'Collaboration', 'Leadership', 'Presentation Skills', 'Analytical Skills', 'English Proficiency']}]}, 'achievements': None, 'certifications': {'certifications': [{'name': 'Statistics with R', 'by': '', 'link': ''}]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - User data extracted successfully.\n",
      "INFO - User data extracted successfully.\n",
      "INFO - Generating resume JSON from job details and user data.\n",
      "INFO - Processing Resume's EXPERIENCES Section...\n",
      "INFO - Processing Resume's PROJECTS Section...\n",
      "INFO - Processing Resume's SKILLS Section...\n",
      "INFO - Processing Resume's EDUCATIONS Section...\n",
      "INFO - Processing Resume's CERTIFICATIONS Section...\n",
      "INFO - Processing Resume's ACHIEVEMENTS Section...\n",
      "INFO - Resume JSON saved at: output_files/Amazon/JrAppliedScient_20250526141836/resume.json\n",
      "INFO - Resume JSON generated successfully.\n",
      "INFO - Resume JSON validated successfully.\n",
      "INFO - Converting resume JSON to LaTeX format.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'personal': {'name': 'Aaron Dankert', 'phone': '(505)507{-}8207', 'email': 'aarondankert@gmail.com', 'linkedin': 'https://www.linkedin.com/in/aadank/', 'github': 'https://github.com/aa{-}dank'}, 'summary': None, 'experiences': {'work_experience': [{'role': 'Systems Analyst', 'company': 'University of California, Santa Cruz', 'location': 'Santa Cruz, CA, USA', 'from_date': '2023', 'to_date': 'Present', 'description': ['Enhanced the department’s online presence by managing multiple websites and developing new web assets, resulting in increased user engagement.', 'Led a successful migration from CMiC Project Management Software to FileMaker, authoring scripts for data extraction and integration, which improved data workflows and accuracy.', 'Authored over 10,000 words of technical documentation and implemented advanced data cleaning techniques, facilitating improved process understanding, compliance, and data integrity across teams.']}, {'role': 'Archives Manager', 'company': 'University of California, Santa Cruz', 'location': 'Santa Cruz, CA, USA', 'from_date': '2020', 'to_date': '2023', 'description': ['Streamlined file management and access by developing a comprehensive Flask{-}based archives application, reducing retrieval overhead by 40\\\\%.', 'Enhanced database accuracy and security by implementing data scraping and Google authentication, ensuring compliance and robust data protection.', 'Led and trained a team of six employees on data management best practices and software tools, increasing team efficiency through targeted support and fostering a collaborative environment.']}, {'role': 'Data Coordinator', 'company': 'University of California, Santa Cruz', 'location': 'Santa Cruz, CA, USA', 'from_date': '2017', 'to_date': '2020', 'description': ['Supported over \\\\$900 million in capital projects by providing project engineering support, ensuring regulatory compliance, and enhancing system performance through effective data management.', 'Improved data quality and integrity by leveraging and optimizing existing tools, ensuring 100\\\\% compliance with regulatory standards and managing sensitive data securely.', 'Generated actionable insights through advanced statistical data processing and optimized financial tracking and reporting workflows, improving departmental efficiency.']}]}, 'projects': {'projects': [{'name': 'LLM Attribution: Challenges and Insights Across Model Stochasticity', 'type': None, 'link': '', 'resources': None, 'from_date': '2023', 'to_date': '2025', 'description': ['Engineered a text classification system by implementing Random Forest and Logistic Regression models with scikit{-}learn and PyTorch, improving LLM{-}generated text attribution accuracy.', 'Developed scalable data processing pipelines integrating BERT embeddings with custom stylometric features, enabling robust multi{-}class attribution and efficient large{-}scale data handling.', 'Conducted advanced data analysis and visualization using t{-}SNE, PCA, and statistical methods, generating actionable insights and comprehensive model performance reports.']}, {'name': 'Archives App', 'type': None, 'link': '', 'resources': None, 'from_date': '', 'to_date': '', 'description': ['Developed a data{-}driven archives application utilizing Python, Flask, and PostgreSQL, streamlining archival file management and enhancing data organization.', 'Integrated Google authentication, role{-}based access control, and Redis for background tasks, improving scalability, responsiveness, and organizational compliance.', 'Designed RESTful API endpoints and intuitive web interfaces, automating data interactions and reducing manual handling by 60\\\\%.']}, {'name': 'Archiving Desktop Application', 'type': None, 'link': '', 'resources': None, 'from_date': 'Sep 2023', 'to_date': 'Dec 2023', 'description': ['Developed a standalone desktop application with an intuitive PySimpleGUI interface and SQLite integration, improving document archiving workflow efficiency by over 50\\\\%.', 'Implemented robust backup solutions and automated file tracking, reducing data loss risk, enhancing data security, and decreasing manual archiving time by approximately 35\\\\%.', 'Enhanced tracking and reporting accuracy by 25\\\\% through comprehensive process automation and optimized data management.']}, {'name': 'LLM Research Benchmarking Platform', 'type': None, 'link': '', 'resources': None, 'from_date': '2024', 'to_date': '2025', 'description': ['Architected a scalable data pipeline extracting research papers from five academic databases, engineered a PostgreSQL database with vector embedding storage for 10,000+ papers, and enabled semantic search with pgvector.', 'Implemented advanced RAG (Retrieval{-}Augmented Generation) architecture and evaluation frameworks, reducing processing time by 40\\\\% and enhancing LLM scientific evaluation capabilities.', 'Developed interactive dashboards and comprehensive error analysis protocols, enabling reproducible benchmarking and actionable insights across scientific domains.']}, {'name': 'Job Hunter Toolbox', 'type': None, 'link': '', 'resources': None, 'from_date': '2024', 'to_date': '', 'description': ['Architected an automated resume generation system using Python, LaTeX, and LLM services, reducing application preparation time by nearly 60\\\\%.', 'Engineered a modular resume management system leveraging Jinja2 templates and LaTeX for pixel{-}perfect PDF generation and consistent formatting.', 'Implemented intelligent content tailoring and automated keyword optimization using LLM APIs, enhancing resume ATS compatibility and application relevance.']}]}, 'educations': {'education': [{'degree': 'Masters in Data Science', 'university': 'University of Michigan, Ann Arbor, USA', 'from_date': '2023', 'to_date': '2025', 'courses': []}, {'degree': 'Bachelors in Psychology and Economics', 'university': 'University of New Mexico, Albuquerque, USA', 'from_date': '2006', 'to_date': '2010', 'courses': []}]}, 'skills': {'skill_section': [{'name': 'Programming Languages', 'skills': ['Python', 'R', 'SQL', 'Typescript', 'Javascript', 'C++']}, {'name': 'Data Science \\\\& Machine Learning', 'skills': ['Pandas', 'Numpy', 'Scikit{-}Learn', 't{-}SNE', 'Principal Component Analysis', 'Pyspark', 'Statistical Analysis', 'Forecasting']}, {'name': 'Visualization', 'skills': ['Seaborn', 'Matplotlib', 'Altair', 'Jupyter Notebooks']}, {'name': 'Database Technologies', 'skills': ['Postgresql', 'SQLAlchemy', 'Redis', 'Sqlite']}, {'name': 'Operating Systems', 'skills': ['CentOS', 'Ubuntu', 'Windows', 'MacOS']}, {'name': 'Language Models \\\\& NLP', 'skills': ['OpenAI ChatGPT', 'NLTK', 'BERT']}, {'name': 'Tools \\\\& Technologies', 'skills': ['Flask', 'Jupyter Notebooks', 'Pyspark']}, {'name': 'Cloud \\\\& DevOps', 'skills': ['AWS', 'Azure Cognitive Services', 'Azure Services', 'Azure Cognitive and Generative AI', 'GCP', 'Cloud', 'Security Engineering', 'Security Architecture', 'DevOps', 'CI/CD']}, {'name': 'Software Development \\\\& Full Stack', 'skills': ['Java', 'React', 'JavaScript', 'Frontend', 'Backend', 'HTML', 'CSS', 'API', 'Angular', 'Full Stack Software Development']}, {'name': 'AI/ML Engineering', 'skills': ['Artificial Intelligence', 'Machine Learning', 'Deep Learning', 'Pretrained Models', 'AutoML', 'Data Extraction', 'Data Transformation', 'Data Loading', 'ETL', 'Data Modeling', 'Data Warehouse']}, {'name': 'Data Governance \\\\& Security', 'skills': ['Data Quality', 'Data Governance', 'Data Privacy', 'Compliance', 'SLA', 'Security', 'Privacy']}, {'name': 'Soft Skills', 'skills': ['Problem{-}solving', 'Communication skills', 'Cross{-}functional Collaboration', 'Collaboration', 'Leadership', 'Presentation Skills', 'Analytical Skills', 'English Proficiency']}]}, 'achievements': None, 'certifications': {'certifications': [{'name': 'Statistics with R', 'by': '', 'link': ''}]}}\n",
      "<Template 'less_basic_template.tex'>\n",
      "/Users/aaronrdankert/projects/job_hunter_toolbox/templates/less_basic_template.tex\n",
      "%==== PACKAGES AND OTHER DOCUMENT CONFIGURATIONS  ====%\n",
      "\\documentclass{resume}\n",
      "\n",
      "% Document settings\n",
      "\\usepackage[left=0.25in,top=0.25in,right=0.25in,bottom=0.25in]{geometry}\n",
      "\n",
      "% Font and text packages (remove duplicates)\n",
      "\\usepackage[T1]{fontenc}\n",
      "\\usepackage{lmodern}\n",
      "\\usepackage{textcomp}\n",
      "\\usepackage{fontawesome}\n",
      "\n",
      "% Color and hyperlink setup\n",
      "\\usepackage{xcolor}\n",
      "\\usepackage{hyperref}\n",
      "\n",
      "% Define custom colors\n",
      "\\definecolor{myblue}{RGB}{0, 164, 218}\n",
      "\n",
      "% Hyperlink configuration\n",
      "\\hypersetup{\n",
      "    colorlinks=true,\n",
      "    linkcolor=myblue,\n",
      "    citecolor=myblue,   \n",
      "    urlcolor=myblue\n",
      "}\n",
      "\n",
      "% Document formatting\n",
      "\\linespread{1.1}\n",
      "\\renewcommand{\\labelitemi}{\\textcolor{myblue}{$\\bullet$}}\n",
      "\\setlength{\\parskip}{0.5em}\n",
      "\n",
      "%==== Headings ====%\n",
      "\\name{Aaron Dankert} % Your name\n",
      "\\address{\n",
      "{\\faPhone} \\href{tel:(505)507{-}8207}{(505)507{-}8207} \\quad {\\faEnvelope} \\href{mailto:aarondankert@gmail.com}{aarondankert@gmail.com} \\quad {\\faGithub} \\href{https://github.com/aa{-}dank}{https://github.com/aa{-}dank} \\quad {\\faLinkedin} \\href{https://www.linkedin.com/in/aadank/}{https://www.linkedin.com/in/aadank/} }\n",
      "\n",
      "% Start document content\n",
      "\\begin{document}\n",
      "\n",
      "%===== WORK EXPERIENCE SECTION =====%\n",
      "\n",
      "%==== EDUCATION SECTION ====%\n",
      "\n",
      "% ==== PROJECTS SECTION =====%\n",
      "    \\begin{rSection}{Projects}\n",
      "                    \\begin{rSubsection}\n",
      "                                    {}\n",
      "                                {\\normalfont{ - }}{}{}\n",
      "                            \\end{rSubsection}\n",
      "            \\end{rSection}\n",
      "\n",
      "%==== TECHNICAL STRENGTHS SECTION ====%\n",
      " \n",
      "\n",
      "% ACHIEVEMENTS SECTION\n",
      "\n",
      "\\newcommand\\myfontsize{\\fontsize{0.1pt}{0.1pt}\\selectfont} \\myfontsize \\color{white}\n",
      ", , {artificial intelligence engineer, azure cognitive services exp, azure services, core azure services, azure cognitive and generative ai, genai, aws,  gcp, java, clean, efficient, maintainable code, react, front end, back end, ai solutions, data analysis, pretrained models, automl, software development principles, version control, testing, continuous integration and deployment, python, javascript, prompt engieering, frontend, backend, html, css, api, angular, development, machine learning, artificial intelligence, deep learning, data warehouse, data modeling, data extraction, data transformation, data loading, sql, etl, data quality, data governance, data privacy, data visualization, data controls, privacy, security, compliance, sla, aws, terabyte to petabyte scale data, full stack software development, cloud, security engineering, security architecture, ai/ml engineering, technical product management, microsoft office, google suite, visualization tools, scripting, coding, programming languages, analytical skills, collaboration, leadership, communication, presentation skills, computer vision, senior, ms or ph.d., 3d pose estimation, slam, robotics, object tracking, real-time systems, scalability, autonomy, robotic process automation, java, go, matlab, devops, ci/cd, programming, computer vision, data science, machine learning frameworks, deep learning toolsets, problem-solving, individual contributor, statistics, risk assessments, statistical modeling, apis, technical discussions, cross-functional teams}\n",
      "\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from creds import OPENAI_KEY\n",
    "from application_generator import JobApplicationBuild, JobApplicationBuilder\n",
    "from utils import LatexToolBox, text_to_pdf\n",
    "from models import ChatGPT\n",
    "from prompts.resume_section_prompts import RESUME_WRITER_PERSONA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "logger.info(\"Setting up LLM and JobApplicationBuilder.\")\n",
    "llm = ChatGPT(\n",
    "    api_key=OPENAI_KEY,\n",
    "    model=\"gpt-4.1\",\n",
    "    system_prompt=RESUME_WRITER_PERSONA,\n",
    "    max_output_tokens=None,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "build = JobApplicationBuild(\n",
    "    resume_tex_template_path=\"less_basic_template.tex\",\n",
    "    resume_cls_path=\"less_basic_template.cls\",\n",
    "    job_content_path=\"input_data/Jr Applied Scientist - Santa Cruz - Job ID_ 2881047.pdf\",\n",
    "    user_details_content_path=r\"input_data/full_resume_contents_20250524.pdf\"\n",
    ")\n",
    "\n",
    "generator = JobApplicationBuilder(\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "try:\n",
    "    build = generator.extract_job_content(build)\n",
    "    logger.info(\"Job content extracted successfully.\")\n",
    "\n",
    "    build = generator.user_data_extraction(build)\n",
    "    logger.info(\"User data extracted successfully.\")\n",
    "\n",
    "    build = generator.generate_resume_json(build)\n",
    "    logger.info(\"Resume JSON generated successfully.\")\n",
    "\n",
    "    build = generator.validate_resume_json(build)\n",
    "    logger.info(\"Resume JSON validated successfully.\")\n",
    "\n",
    "    build = generator.resume_json_to_resume_tex(build)\n",
    "    logger.info(\"Resume TeX file generated successfully.\")\n",
    "\n",
    "    custom_letter_instructions = input(\"Enter any custom application instructions for the cover letter: \")\n",
    "    build = generator.generate_cover_letter(build, custom_letter_instructions, need_pdf=False)\n",
    "    logger.info(\"Cover letter generated successfully.\")\n",
    "\n",
    "    resume_tex_fonts, _ = LatexToolBox.extract_tex_font_dependencies(build.resume_tex_path)\n",
    "    font_statuses = LatexToolBox.check_fonts_installed(resume_tex_fonts)\n",
    "    if not all([v for k, v in font_statuses.items()]):\n",
    "        for k, v in font_statuses.items():\n",
    "            if not v:\n",
    "                logger.warning(f\"{k} not installed\")\n",
    "\n",
    "    generator.cleanup_files(build)\n",
    "    logger.info(\"Temporary files cleaned up successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the resume latex and cover letter text before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = LatexToolBox.compile_latex_to_pdf(\n",
    "    tex_filepath=build.resume_tex_path, \n",
    "    cls_filepath=build.resume_cls_path,\n",
    "    output_destination_path=build.output_destination\n",
    ")\n",
    "if success:\n",
    "    # Remove auxiliary files using resume base name (without extension)\n",
    "    base_name = os.path.splitext(os.path.basename(build.resume_tex_path))[0]\n",
    "    LatexToolBox.cleanup_latex_files(build.output_destination, base_name)\n",
    "    logger.info(f\"Resume PDF is saved at {build.resume_tex_path.replace('.tex','.pdf')}\")\n",
    "else:\n",
    "    logger.error(\"LaTeX compilation failed.\")\n",
    "\n",
    "cover_letter_pdf_path = build.cover_letter_path.replace('.txt', '.pdf')\n",
    "cover_letter_pdf_path = text_to_pdf(build.cover_letter_path, cover_letter_pdf_path)\n",
    "logger.info(f\"Cover Letter PDF is saved at {cover_letter_pdf_path}\")\n",
    "\n",
    "generator.cleanup_files(build)\n",
    "logger.info(\"Temporary files cleaned up successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the job content file to the output folder\n",
    "shutil.move(job_content_path, generator.get_job_doc_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the resume latex and cover letter text before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LatexToolBox.compile_latex_to_pdf(tex_filepath=resume_tex_path)\n",
    "logger.info(f\"Resume PDF is saved at {resume_tex_path.replace('.tex', '.pdf')}\")\n",
    "\n",
    "cover_letter_pdf_path = cover_letter_txt_path.replace('.txt', '.pdf')\n",
    "cover_letter_pdf_path = text_to_pdf(cover_letter, cover_letter_pdf_path)\n",
    "logger.info(f\"Cover Letter PDF is saved at {cover_letter_pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the job content file to the output folder\n",
    "shutil.move(job_content_path, generator.get_job_doc_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from creds import OPENAI_KEY\n",
    "from application_generator import JobApplicationBuilder\n",
    "from application_generator import JobApplicationBuilder\n",
    "from utils import LatexToolBox, text_to_pdf\n",
    "from models import ChatGPT\n",
    "from prompts.resume_section_prompts import RESUME_WRITER_PERSONA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "p = r\"output_files/MolinaHealthcare/DataAnalystSqlD_20250219194427/resume.tex\"\n",
    "logger.info(f\"Checking existence of file: {p}\")\n",
    "logger.info(f\"File exists: {os.path.exists(p)}\")\n",
    "\n",
    "LatexToolBox.compile_latex_to_pdf(tex_filepath=p,\n",
    "                                  cls_filepath=\"templates/less_basic_template.cls\",\n",
    "                                  output_destination_path=\"output_files/MolinaHealthcare/DataAnalystSqlD_20250219194427\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from creds import OPENAI_KEY\n",
    "from application_generator import JobApplicationBuild, JobApplicationBuilder\n",
    "from utils import LatexToolBox, text_to_pdf\n",
    "from models import ChatGPT\n",
    "from prompts.resume_section_prompts import RESUME_WRITER_PERSONA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "success = LatexToolBox.compile_latex_to_pdf(\n",
    "    tex_filepath='output_files/Ascendion/PythonEngineerI_20250507212335/resume.tex',\n",
    "    cls_filepath='templates/less_basic_template.cls',\n",
    "    output_destination_path='output_files/Ascendion/PythonEngineerI_20250507212335')\n",
    "if success:\n",
    "    logger.info(\"LaTeX compilation succeeded.\")\n",
    "else:\n",
    "    logger.error(\"LaTeX compilation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import LatexToolBox\n",
    "from logger import setup_logger\n",
    "\n",
    "# Initialize logger if not already done in the notebook\n",
    "if 'logger' not in globals():\n",
    "    logger = setup_logger(name=\"NotebookCompileLogger\", notebook=True)\n",
    "    logger.info(\"Logger initialized for master.tex compilation.\")\n",
    "\n",
    "# Define file paths\n",
    "templates_dir = \"templates\"\n",
    "master_tex_path = os.path.join(templates_dir, \"master.tex\")\n",
    "resume_cls_path = os.path.join(templates_dir, \"resume.cls\") # master.tex uses resume.cls\n",
    "output_dir = templates_dir # Output PDF in the same directory\n",
    "\n",
    "logger.info(f\"Attempting to compile {master_tex_path} to PDF in {output_dir}.\")\n",
    "logger.info(f\"Using CLS file: {resume_cls_path}\")\n",
    "\n",
    "# Ensure the cls file specified in master.tex is resume.cls\n",
    "# The compile_latex_to_pdf function handles copying the cls file with the correct name.\n",
    "\n",
    "success = LatexToolBox.compile_latex_to_pdf(\n",
    "    tex_filepath=master_tex_path,\n",
    "    cls_filepath=resume_cls_path, # Provide the actual path to resume.cls\n",
    "    output_destination_path=output_dir\n",
    ")\n",
    "\n",
    "if success:\n",
    "    pdf_output_path = os.path.join(output_dir, \"master.pdf\")\n",
    "    logger.info(f\"LaTeX compilation of {master_tex_path} succeeded. PDF saved at {pdf_output_path}\")\n",
    "    # Clean up auxiliary files\n",
    "    base_name = os.path.splitext(os.path.basename(master_tex_path))[0]\n",
    "    LatexToolBox.cleanup_latex_files(output_dir, base_name)\n",
    "    logger.info(f\"Auxiliary files for {base_name} cleaned up from {output_dir}.\")\n",
    "else:\n",
    "    logger.error(f\"LaTeX compilation of {master_tex_path} failed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
